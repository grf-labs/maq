% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/maq.R
\name{maq}
\alias{maq}
\title{Fit a Multi-Armed Qini.}
\usage{
maq(
  reward,
  cost,
  budget,
  DR.scores,
  target.with.covariates = TRUE,
  R = 0,
  paired.inference = TRUE,
  sample.weights = NULL,
  clusters = NULL,
  tie.breaker = NULL,
  num.threads = NULL,
  seed = 42
)
}
\arguments{
\item{reward}{A matrix of reward estimates.}

\item{cost}{A matrix of cost estimates. If the costs are the same for each unit, then this can also
be a ncol(reward)-length vector.}

\item{budget}{The maximum spend per unit to fit the MAQ path on.}

\item{DR.scores}{A matrix of rewards to evaluate the MAQ on. For valid statistical inference, the
reward and cost estimates should be obtained independently from this evaluation data.}

\item{target.with.covariates}{If TRUE, then the optimal policy takes covariates into
account. If FALSE, then the optimal policy only takes the average reward and cost into account when
allocating treatment. Default is TRUE.}

\item{R}{Number of bootstrap replicates for computing standard errors. Default is 0
(only point estimates are computed).}

\item{paired.inference}{Whether to allow for paired tests with other cost curves. If TRUE (Default)
then the path of bootstrap replicates are stored in order to perform paired comparisons.
This takes memory on the order of O(RnK) and requires the comparison objects to be fit with the
same seed and R values as well as the same number of samples.}

\item{sample.weights}{Weights given to an observation in estimation.
If NULL, each observation is given the same weight. Default is NULL.}

\item{clusters}{Vector of integers or factors specifying which cluster each observation corresponds to.
Default is NULL (ignored).}

\item{tie.breaker}{An optional permutation of the the integers 1 to nrow(rewards) used to
break potential ties in the optimal treatment allocation. If NULL, the ties are broken by
the lowest sample id (i.e. the sample appearing first in the data). Default is NULL.}

\item{num.threads}{Number of threads used in bootstrap replicates. By default, the number of threads
is set to the maximum hardware concurrency.}

\item{seed}{The seed of the C++ random number generator. Default is 42.}
}
\value{
A fit maq object.
}
\description{
Fit a Multi-Armed Qini.
}
\examples{
\donttest{
if (require("grf", quietly = TRUE)) {

# Fit a CATE estimator (using GRF) on a training sample.
n <- 3000
p <- 5
X <- matrix(runif(n * p), n, p)
W <- as.factor(sample(c("A", "B", "C"), n, replace = TRUE))
Y <- X[, 1] + X[, 2] * (W == "B") + 1.5 * X[, 3] * (W == "C") + rnorm(n)
train <- sample(1:n, n/2)
eval <- -train

tau.forest <- grf::multi_arm_causal_forest(X[train, ], Y[train], W[train])

# Predict CATEs on held out evaluation data.
tau.hat <- predict(tau.forest, X[eval, ], drop = TRUE)$predictions

# Form cost estimates - the following are a toy example.
cost.hat <- cbind(X[eval, 4] / 4, X[eval, 5])

# Fit an evaluation forest to compute doubly robust evaluation set scores.
eval.forest <- grf::multi_arm_causal_forest(X[eval, ], Y[eval], W[eval])
DR.scores <- grf::get_scores(eval.forest, drop = TRUE)

# Fit a MAQ with evaluation set estimates using 200 bootstrap replicates for confidence intervals.
max.budget <- 1
mq <- maq(tau.hat, cost.hat, max.budget, DR.scores, R = 200)

# Plot the MAQ curve.
plot(mq)
legend("topleft", c("All arms", "95\% CI"), lty = c(1, 3))

# Get an estimate of optimal reward at a given spend per unit along with standard errors.
average_gain(mq, spend = 0.2)

# Get the optimal treatment allocation matrix at a given spend per unit.
pi.mat <- predict(mq, spend = 0.2)

# If the treatment randomization probabilities are known, then an alternative to
# evaluation via AIPW scores is to use inverse-propensity weighting (IPW).
W.hat <- rep(1/3, 3)
observed.W <- match(W, levels(W))
Y.mat <- matrix(0, length(W), nlevels(W))
Y.mat[cbind(seq_along(observed.W), observed.W)] <- Y
Y.ipw <- sweep(Y.mat, 2, W.hat, "/")
Y.ipw.eval <- Y.ipw[eval, -1] - Y.ipw[eval, 1]

mq.ipw <- maq(tau.hat, cost.hat, max.budget, Y.ipw.eval)
plot(mq.ipw, add = TRUE, col = 2)
legend("topleft", c("All arms", "95\% CI", "All arms (IPW)"), col = c(1, 1, 2), lty = c(1, 3, 1))

# Estimate some baseline policies.
# a) A policy that ignores covariates and only only takes the average reward/cost into account.
mq.avg <- maq(tau.hat, cost.hat, max.budget, DR.scores, target.with.covariates = FALSE, R = 200)

# b) A policy that only use arm 1.
mq.arm1 <- maq(tau.hat[, 1], cost.hat[, 1], max.budget, DR.scores[, 1], R = 200)

# c) A policy that only use arm 2.
mq.arm2 <- maq(tau.hat[, 2], cost.hat[, 2], max.budget, DR.scores[, 2], R = 200)

plot(mq, ci.args = NULL)
plot(mq.avg, col = 2, add = TRUE, ci.args = NULL)
plot(mq.arm1, col = 3, add = TRUE, ci.args = NULL)
plot(mq.arm2, col = 4, add = TRUE, ci.args = NULL)
legend("topleft", c("All arms (targeting)", "All arms (without targeting)", "Arm 1", "Arm 2"),
       col = 1:4, lty = 1)

# Estimate the value of employing all arms over a random allocation.
difference_gain(mq, mq.avg, spend = 0.2)

# Estimate the value of adding arm 1 to the optimal policy mix.
difference_gain(mq, mq.arm1, spend = 0.2)

# Estimate the value of adding arm 2 to the optimal policy mix.
difference_gain(mq, mq.arm2, spend = 0.2)

}
}

}
