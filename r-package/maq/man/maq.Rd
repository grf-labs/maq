% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/maq.R
\name{maq}
\alias{maq}
\title{Fit a Multi-Action Qini.}
\usage{
maq(
  reward,
  cost,
  budget,
  reward.scores,
  R = 200,
  sample.weights = NULL,
  clusters = NULL,
  tie.breaker = NULL,
  num.threads = NULL,
  seed = runif(1, 0, .Machine$integer.max)
)
}
\arguments{
\item{reward}{A matrix of reward estimates.}

\item{cost}{A matrix of cost estimates. If the costs are the same for each unit, then this can also
be a ncol(reward)-length vector.}

\item{budget}{The maximum spend per unit to fit the MAQ path on.}

\item{reward.scores}{A matrix of rewards to evaluate the MAQ on.}

\item{R}{Number of bootstrap replicates for SEs. Default is 200.}

\item{sample.weights}{Weights given to an observation in estimation.
If NULL, each observation is given the same weight. Default is NULL.}

\item{clusters}{Vector of integers or factors specifying which cluster each observation corresponds to.
Default is NULL (ignored).}

\item{tie.breaker}{An optional permutation of the the integers 1 to nrow(rewards) used to
break potential ties in the optimal treatment allocation. If NULL, the ties are broken by
the lowest sample id (i.e. the sample appearing first in the data). Default is NULL.}

\item{num.threads}{Number of threads used in bootstrap replicates. By default, the number of threads
is set to the maximum hardware concurrency.}

\item{seed}{The seed of the C++ random number generator.}
}
\value{
A fit maq object.
}
\description{
Fit a Multi-Action Qini.
}
\examples{
\donttest{
if (require("grf", quietly = TRUE)) {
# Fit a CATE estimator (using GRF) on a training sample.
n <- 2000
p <- 5
X <- matrix(runif(n * p), n, p)
W <- as.factor(sample(c("A", "B", "C"), n, replace = TRUE))
Y <- X[, 1] + X[, 2] * (W == "B") + X[, 3] * (W == "C") + rnorm(n)
train <- sample(1:n, n/2)
eval <- -train

tau.forest <- grf::multi_arm_causal_forest(X[train, ], Y[train], W[train])

# Predict CATEs on held out evaluation data.
tau.hat <- predict(tau.forest, X[eval, ])$predictions[,,]

# Form cost estimates - the following are a toy example.
cost.hat <- X[eval, 4:5]

# Fit an evaluation forest to compute doubly robust evaluation set scores.
eval.forest <- grf::multi_arm_causal_forest(X[eval, ], Y[eval], W[eval])
DR.scores <- grf::get_scores(eval.forest)[,,]

# Fit a MAQ using evaluation set estimates.
max.budget <- 1
mq <- maq(tau.hat, cost.hat, max.budget, DR.scores)

# Plot the MAQ curve.
plot(mq)

# Get an estimate of optimal reward at a given spend per unit along with standard errors.
average_gain(mq, spend = 0.3)

# Get the optimal treatment allocation matrix at a given spend per unit.
pi.mat <- predict(mq, spend = 0.3)

# If the treatment randomization probabilities are known, then an alternative to
# evaluation via AIPW scores is to use inverse-propensity weighting (IPW).
W.hat.true <- rep(1/3, 3)
observed.W <- match(W, levels(W))
Y.k.mat <- matrix(0, length(W), nlevels(W))
Y.k.mat[cbind(seq_along(observed.W), observed.W)] <- Y
Y.k.ipw <- sweep(Y.k.mat, 2, W.hat.true, "/")
Y.k.ipw.eval <- Y.k.ipw[eval, -1] - Y.k.ipw[eval, 1]

mq.ipw <- maq(tau.hat, cost.hat, max.budget, Y.k.ipw.eval)

plot(mq.ipw, col = 2, add = TRUE)
legend("topleft", c("DR", "IPW"), col = 1:2, lty = 1, bty = "n")
}
}

}
